{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 1"
      ],
      "metadata": {
        "id": "E6hk7gs1vdDy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQMlvRmauHcy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "vector_a=tf.constant([1,2,3],dtype=tf.float32)\n",
        "vector_b=tf.constant([4,5,6],dtype=tf.float32)\n",
        "result=tf.add(vector_a,vector_b)\n",
        "result1=tf.subtract(vector_a,vector_b)\n",
        "result2=tf.multiply(vector_a,vector_b)\n",
        "result3=tf.divide(vector_a,vector_b)\n",
        "print(result.numpy())\n",
        "print(result1.numpy())\n",
        "print(result2.numpy())\n",
        "print(result3.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 2"
      ],
      "metadata": {
        "id": "hz8TRbP6vb4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generating some synthetic data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(1000, 5)\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] - 5 * X[:, 2] + np.random.randn(1000)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Define the model using the Input layer\n",
        "model = Sequential()\n",
        "model.add(Input(shape=(X_train.shape[1],)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(units=1))  # Output layer for regression (no activation function)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error on Test Set: {mse}\")\n",
        "\n",
        "# Plot the training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IHf795TnwTo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP:3"
      ],
      "metadata": {
        "id": "x6kam7qbwrb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Input\n",
        "x_train=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y_train=np.array([[0],[0],[0],[1]])\n",
        "model=Sequential()\n",
        "\n",
        "model.add(Dense(64, activation=\"relu\",input_shape=(x_train.shape[1],)))\n",
        "model.add(Dense(32,activation=\"relu\"))\n",
        "model.add(Dense(1))\n",
        "model.compile(optimizer='sgd', loss='binary_crossentropy',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=5,verbose=1)\n",
        "y_pred=model.predict(x_train)\n",
        "print(\"Test INput: \",x_train)\n",
        "print(\"Predicted Output: \",y_pred)"
      ],
      "metadata": {
        "id": "8DRUSQp8vkXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp:4"
      ],
      "metadata": {
        "id": "CbYimMRy-5n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "input_size=100\n",
        "hidden_units=32\n",
        "output_units=1\n",
        "\n",
        "\n",
        "\n",
        "model=keras.Sequential([\n",
        "    keras.layers.Input(shape=(input_size,)),\n",
        "    keras.layers.Dense(units=hidden_units,activation='sigmoid'),\n",
        "    keras.layers.Dense(units=output_units,activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "aCDcDztu-5Os",
        "outputId": "5c1edf93-5fa6-42d6-90df-b7579583213a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_22 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │           \u001b[38;5;34m3,232\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m33\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">3,232</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_23 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,265\u001b[0m (12.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,265</span> (12.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,265\u001b[0m (12.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,265</span> (12.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp:5"
      ],
      "metadata": {
        "id": "MsP8LpgiE69U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,Input,Dense,Flatten,MaxPooling2D\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
        "x_train=x_train.reshape(-1,28,28,1).astype('float32')/255\n",
        "x_test=x_test.reshape(-1,28,28,1).astype('float32')/255\n",
        "\n",
        "\n",
        "y_train=to_categorical(y_train,10)\n",
        "y_test=to_categorical(y_test,10)\n",
        "\n",
        "\n",
        "\n",
        "model= Sequential(\n",
        "    [\n",
        "        Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(28,28,1)),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Conv2D(64,kernel_size=(3,3),activation='relu'),\n",
        "MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128,activation='relu'),\n",
        "        Dense(10,activation='softmax')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=128,verbose=1,validation_split=0.2,)\n",
        "test_loss,test_acc=model.evaluate(x_test,y_test,verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38xYs0ZFE-Yz",
        "outputId": "8ec230b9-01a8-4a02-eef5-53247a8a4720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP:6"
      ],
      "metadata": {
        "id": "6sXMNuIUv6zZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Initial model training function\n",
        "def build_and_train_model(learning_rate, batch_size, epochs):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D(pool_size=(2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
        "    test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "    return test_accuracy\n",
        "\n",
        "# Hyperparameter tuning\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "batch_sizes = [32, 64, 128]\n",
        "epochs = 10\n",
        "\n",
        "best_accuracy = 0\n",
        "best_hyperparameters = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for bs in batch_sizes:\n",
        "        accuracy = build_and_train_model(lr, bs, epochs)\n",
        "        print(f'Learning Rate: {lr}, Batch Size: {bs}, Accuracy: {accuracy}')\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_hyperparameters = {'learning_rate': lr, 'batch_size': bs}\n",
        "\n",
        "# Final model training with best hyperparameters\n",
        "final_model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "optimizer = Adam(learning_rate=best_hyperparameters['learning_rate'])\n",
        "final_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "final_model.fit(x_train, y_train, batch_size=best_hyperparameters['batch_size'], epochs=epochs)\n",
        "final_test_loss, final_test_accuracy = final_model.evaluate(x_test, y_test)\n",
        "print(f'Final Model Test Accuracy: {final_test_accuracy}')\n"
      ],
      "metadata": {
        "id": "BRo1J-8kv90q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP 7 and 8"
      ],
      "metadata": {
        "id": "4iwd8_fCM3-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess the dataset (CIFAR-10)\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "# Load pre-trained VGG16 model without the top layer\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(32, 32, 3))\n",
        "\n",
        "# Freeze the layers of the pre-trained model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom layers on top of the pre-trained model\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "cSrOTH_ixL_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exp 9:"
      ],
      "metadata": {
        "id": "pxgYdqBWWPzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # Corrected import\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "from tensorflow.keras.optimizers import Adam  # Explicitly import Adam optimizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# Example using IMDb movie reviews dataset\n",
        "from tensorflow.keras.datasets import imdb\n",
        "\n",
        "# Number of words to consider as features\n",
        "num_words = 10000\n",
        "\n",
        "# Load the dataset\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
        "\n",
        "# Pad sequences to ensure uniform input length\n",
        "max_length = 500\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# Build the RNN model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=num_words, output_dim=64, input_length=max_length),\n",
        "    SimpleRNN(64, return_sequences=False),\n",
        "    Dense(1, activation='sigmoid')  # Corrected the quote marks here\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])  # Fixed quotes here too\n",
        "\n",
        "# Train the model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy}')\n"
      ],
      "metadata": {
        "id": "_uJ3WY54W4MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EXP 10"
      ],
      "metadata": {
        "id": "_YpMu3RzpoHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU, BatchNormalization, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "# Example using MNIST dataset\n",
        "(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "# Define the Generator model\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(256, input_dim=100),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(1024),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "        Dense(28*28*1, activation='tanh'),\n",
        "        Reshape((28, 28, 1))\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Define the Discriminator model\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28, 1)),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Compile the Discriminator\n",
        "discriminator = build_discriminator()\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
        "\n",
        "# Compile the combined GAN model\n",
        "generator = build_generator()\n",
        "z = tf.keras.Input(shape=(100,))\n",
        "img = generator(z)\n",
        "discriminator.trainable = False\n",
        "valid = discriminator(img)\n",
        "combined = tf.keras.Model(z, valid)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
        "\n",
        "# Train the GAN\n",
        "def train_gan(epochs, batch_size=128, save_interval=200):\n",
        "    # Load and preprocess the dataset\n",
        "    (x_train, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # ---------------------\n",
        "        # Train Discriminator\n",
        "        # ---------------------\n",
        "        # Select a random half batch of images\n",
        "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
        "        imgs = x_train[idx]\n",
        "\n",
        "        # Sample noise and generate a batch of new images\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        gen_imgs = generator.predict(noise)\n",
        "\n",
        "        # Train the discriminator (real classified as 1 and fake as 0)\n",
        "        d_loss_real = discriminator.train_on_batch(imgs, valid)\n",
        "        d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n",
        "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "        # ---------------------\n",
        "        # Train Generator\n",
        "        # ---------------------\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "        # Train the generator (wants discriminator to mistake images as real)\n",
        "        g_loss = combined.train_on_batch(noise, valid)\n",
        "\n",
        "        # Print the progress\n",
        "        print(f\"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]\")\n",
        "\n",
        "        # If at save interval, save generated image samples\n",
        "        if epoch % save_interval == 0:\n",
        "            save_imgs(epoch)\n",
        "\n",
        "# Generate and save images\n",
        "def save_imgs(epoch):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, 100))\n",
        "    gen_imgs = generator.predict(noise)\n",
        "\n",
        "    # Rescale images 0 - 1\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c)\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "    fig.savefig(f\"images/mnist_{epoch}.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Train the GAN model\n",
        "train_gan(epochs=10000, batch_size=64, save_interval=1000)\n"
      ],
      "metadata": {
        "id": "Ab2g5cH9pYz-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}